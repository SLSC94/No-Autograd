{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Airline Tweets with Vanilla RNN\n",
    "\n",
    "No autograd packages will be used in this implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class parameter:\n",
    "    #base class for all parameters\n",
    "    def __init__(self, data, gradient):\n",
    "        self.data = data\n",
    "        self.grad = gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for simplicity, this RNN cell will be a many-to-one RNN\n",
    "class RNN_cell:\n",
    "    def __init__(self, W_x, W_h, bias):\n",
    "        #our goal is to create a graph dynamically\n",
    "        assert W_x.shape[0] == W_h.shape[0]\n",
    "        self.W_x = parameter(W_x, np.zeros(W_x.shape))\n",
    "        self.W_h = parameter(W_h, np.zeros(W_h.shape))\n",
    "        self.bias = parameter(bias, np.zeros(bias.shape))\n",
    "        \n",
    "        self.forward = {}\n",
    "        self.backward = {}\n",
    "        \n",
    "    def forward_pass(self, h_t_1, x_t):\n",
    "        self.x_t = x_t\n",
    "        self.h_t_1 = h_t_1\n",
    "        self.forward['f1'] = (self.W_h.data @ h_t_1 + self.W_x.data @ x_t) + self.bias.data\n",
    "        self.forward['out'] = np.tanh(self.forward['f1'])\n",
    "        \n",
    "        \n",
    "    def backward_pass(self, in_grad):\n",
    "        self.backward['t1'] = (1 - np.tanh(self.forward['f1'])**2 ) * in_grad\n",
    "        self.backward['out'] = self.W_h.data.T @ self.backward['t1']\n",
    "        \n",
    "        self.W_x.grad = self.backward['t1'] @ self.x_t.T\n",
    "        self.W_h.grad = self.backward['t1'] @ self.h_t_1.T\n",
    "        self.bias.grad = self.backward['t1']\n",
    "    \n",
    "    def update_parameters(self, learning_rate):\n",
    "        self.W_x.data = self.W_x.data - learning_rate * self.W_x.grad\n",
    "        self.W_h.data = self.W_h.data - learning_rate * self.W_h.grad\n",
    "        self.bias.data = self.bias.data - learning_rate * self.bias.grad\n",
    "        \n",
    "        #zero gradients\n",
    "        self.W_x.grad = np.zeros(self.W_x.data.shape)\n",
    "        self.W_h.grad = np.zeros(self.W_h.data.shape)\n",
    "        self.bias.data = np.zeros(self.bias.data.shape)\n",
    "\n",
    "class linear_layer:\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.M = {}\n",
    "        self.M['W'] = parameter(data= 0.1*np.random.normal(size = [out_dim, in_dim]), \n",
    "                                gradient= np.zeros([out_dim, in_dim]))\n",
    "        self.M['b'] = parameter(data= 0.1*np.random.normal(size = [out_dim, 1]), \n",
    "                                gradient= np.zeros([out_dim, 1]))\n",
    "        self.forward = {}\n",
    "        self.backward = {}\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        self.X = X\n",
    "        self.forward['f1'] = np.matmul(self.M['W'].data, self.X)\n",
    "        self.forward['out'] = self.forward['f1'] +  self.M['b'].data\n",
    "        \n",
    "    def backward_pass(self, in_grad):\n",
    "        self.backward['t2'] = in_grad\n",
    "        self.backward['out'] = np.matmul(self.M['W'].data.T, self.backward['t2'])\n",
    "\n",
    "        self.M['W'].grad += np.matmul(self.backward['t2'], self.X.T)\n",
    "        self.M['b'].grad += np.sum(self.backward['t2'], axis = 1, keepdims= True)\n",
    "\n",
    "    \n",
    "    def update_parameters(self, epsilon):\n",
    "        for key in self.M:\n",
    "            \n",
    "            param = self.M[key]\n",
    "            \n",
    "            assert (param.grad).shape == (param.data).shape\n",
    "            \n",
    "            param.data = param.data - epsilon * param.grad/self.forward['f1'].shape[1]\n",
    "            #zero the gradients\n",
    "            param.grad = np.zeros(np.shape(param.grad))\n",
    "\n",
    "class entropylosswithlogits:\n",
    "    def __init__(self):\n",
    "        self.forward = {}\n",
    "        self.backward = {}\n",
    "        self.X = 0\n",
    "        self.y = 0\n",
    "    \n",
    "    def forward_pass(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.forward['f1'] = np.exp(self.X) / np.tile(np.sum(np.exp(self.X), axis=0), (3,1) )\n",
    "        self.forward['f2'] = -y * np.log(self.forward['f1'])\n",
    "        self.forward['out'] = np.sum(self.forward['f2'])\n",
    "        \n",
    "    def backward_pass(self):\n",
    "        self.backward['out'] = self.forward['f1'] - self.y\n",
    "\n",
    "    def update_parameters(self, epsilon):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in our data and preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = pd.read_csv('twitter-airline-sentiment/Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = Tweets[['airline','airline_sentiment','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Airlines = set(Tweets['airline'])\n",
    "list_airlines = []\n",
    "for airline in Airlines:\n",
    "    list_airlines.append(re.sub(\"[^a-zA-Z]\",\"\",airline).lower())\n",
    "    list_airlines.append(re.sub(\"[^a-zA-Z]\",\"\",airline).lower()+'air')\n",
    "list_airlines=set(list_airlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_words(raw_tweet, list_airlines):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \",raw_tweet) \n",
    "    words = letters_only.lower().split()                             \n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    meaningful_words1 = [w for w in meaningful_words if not w in set(list_airlines)]\n",
    "    return( \" \".join( meaningful_words1 )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets['Clean'] = Tweets['text'].apply(tweet_to_words, list_airlines = list_airlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Tweets['Clean'].values.tolist()\n",
    "corpus = [nltk.word_tokenize(tweet) for tweet in X]\n",
    "model = gensim.models.Word2Vec(corpus, min_count = 1, size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_map = {\"neutral\":0, \"positive\":1, \"negative\":2}\n",
    "\n",
    "def onehot(integer, n):\n",
    "    temp = np.zeros((n,1))\n",
    "    temp[integer,0] = 1\n",
    "    return temp\n",
    "\n",
    "Tweets['airline_sentiment_num'] = Tweets['airline_sentiment'].map(sentiment_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Function to help us to clip the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for gradient clipping\n",
    "def restrict_to(X,restriction):\n",
    "    return (np.minimum(X,restriction)* (X > 0) ) + \\\n",
    "            (np.maximum(X,-restriction)* (X < 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "### Note: In this notebook, I am training and testing on the same dataset, as this implementation is unable to deal with words that did not appear in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  loss =  11940.348019767858\n",
      "epoch =  1  loss =  11704.74883545681\n",
      "epoch =  2  loss =  11576.172000177667\n"
     ]
    }
   ],
   "source": [
    "def training(Data, model, epochs, epsilon):\n",
    "    W_h = 0.1*np.random.normal(size = (64,64))\n",
    "    W_x = 0.1*np.random.normal(size = (64,64))\n",
    "    bias = 0.1*np.random.normal(size = (64,1))\n",
    "\n",
    "    \n",
    "    num_examples = Data.shape[0]\n",
    "    \n",
    "    #this is the fully connected layer - this seems confusing....\n",
    "    FCC = linear_layer(in_dim= 64,out_dim=3)\n",
    "    #entropy loss\n",
    "    Loss = entropylosswithlogits()\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for index in range(num_examples):\n",
    "\n",
    "            Sentence = nltk.word_tokenize(Data.loc[index,'Clean'].lower())\n",
    "\n",
    "            label = onehot(Data.loc[index,'airline_sentiment_num'],3)\n",
    "\n",
    "            #initialize the hidden state as a vector of zeros\n",
    "            h_t = np.zeros((64,1))\n",
    "\n",
    "            Dictionary = {}\n",
    "\n",
    "            #This is the forward pass\n",
    "\n",
    "            #########################################################################\n",
    "            for i,word in enumerate(Sentence):\n",
    "\n",
    "                word_rep = model.wv.__getitem__(word)[:,None]\n",
    "                Dictionary[i] = RNN_cell(W_h=W_h, W_x=W_x, bias=bias)\n",
    "                Dictionary[i].forward_pass(h_t_1 = h_t, x_t = word_rep)\n",
    "                h_t = Dictionary[i].forward['out']\n",
    "                \n",
    "            FCC.forward_pass(h_t)\n",
    "            Loss.forward_pass(FCC.forward['out'], y = label)\n",
    "            #########################################################################\n",
    "\n",
    "            #This is the backward pass\n",
    "            Loss.backward_pass()\n",
    "            FCC.backward_pass(Loss.backward['out'])\n",
    "            in_grad = FCC.backward['out']\n",
    "\n",
    "            W_x_grad = 0\n",
    "            W_h_grad = 0\n",
    "            bias_grad = 0\n",
    "\n",
    "            #########################################################################\n",
    "            for i in list(reversed(np.arange(len(Sentence)))):\n",
    "                Dictionary[i].backward_pass(in_grad)\n",
    "                #clip the gradients!\n",
    "                in_grad = restrict_to(Dictionary[i].backward['out'],1)\n",
    "                #we need to sum up all the gradients: This is a little clunky though.\n",
    "                W_x_grad += Dictionary[i].W_x.grad\n",
    "                W_h_grad += Dictionary[i].W_h.grad\n",
    "                bias_grad += Dictionary[i].bias.grad\n",
    "\n",
    "            #########################################################################\n",
    "\n",
    "            W_x -= epsilon * W_x_grad\n",
    "            W_h -= epsilon * W_h_grad\n",
    "            bias -= epsilon * bias_grad\n",
    "\n",
    "            FCC.update_parameters(epsilon)\n",
    "            #add the loss to print out\n",
    "            loss += Loss.forward['out']\n",
    "        print(\"epoch = \",epoch, \" loss = \", loss)\n",
    "        loss = 0\n",
    "    return(FCC, W_h, W_x, bias)\n",
    "        \n",
    "\n",
    "epochs = 3\n",
    "FCC, W_h, W_x, bias = training(Tweets,model,epochs, 0.004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the accuracy of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.6601775956284153\n"
     ]
    }
   ],
   "source": [
    "def test(W_h, W_x, bias, FCC, Data):\n",
    "    num_examples = Data.shape[0]\n",
    "    score = 0\n",
    "    for index in range(num_examples):\n",
    "\n",
    "        Sentence = nltk.word_tokenize(Data.loc[index,'Clean'].lower())\n",
    "\n",
    "        label = onehot(Data.loc[index,'airline_sentiment_num'],3)\n",
    "\n",
    "        #initialize the hidden state as a vector of zeros\n",
    "        h_t = np.zeros((64,1))\n",
    "\n",
    "        Dictionary = {}\n",
    "\n",
    "        #This is the forward pass\n",
    "\n",
    "        #########################################################################\n",
    "        for i,word in enumerate(Sentence):\n",
    "\n",
    "            word_rep = model.wv.__getitem__(word)[:,None]\n",
    "            Dictionary[i] = RNN_cell(W_h=W_h, W_x=W_x, bias=bias)\n",
    "            Dictionary[i].forward_pass(h_t_1 = h_t, x_t = word_rep)\n",
    "            h_t = Dictionary[i].forward['out']\n",
    "\n",
    "        FCC.forward_pass(h_t)\n",
    "        pred = np.argmax(FCC.forward['out'].squeeze() )\n",
    "        actual = np.argmax(label.squeeze())\n",
    "        \n",
    "        score = score + (actual == pred)\n",
    "        \n",
    "        #########################################################################\n",
    "    print('accuracy = ', score/num_examples)\n",
    "test(W_h, W_x, bias, FCC, Tweets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
