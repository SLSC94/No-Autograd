{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is an exercise to delve into the architecture of the CNN.\n",
    "\n",
    "This exercise will only use numpy. No autograd schemes will be used. Backpropogation will be carried out manually to ensure full understanding of the architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start by defining parameter and computation tree classes\n",
    "\n",
    "### Each parameter will keep track of:   \n",
    "1) Its current value  \n",
    "2) The gradient of the loss with respect to itself\n",
    "\n",
    "### Each tree will be made up of a sequence of layers, and each layer will contain:  \n",
    "1) Its parameters (Object)  \n",
    "2) A forward pass (Method)  \n",
    "3) A backward pass to calculate gradients wrt the previous layer (Method)  \n",
    "4) A backward pass to calculate gradients wrt its parameters (Method)  \n",
    "5) A gradient update for its parameters (Method)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class parameter:\n",
    "    #base class for all parameters\n",
    "    def __init__(self, data, gradient):\n",
    "        self.data = data\n",
    "        self.gradient = gradient\n",
    "        \n",
    "\n",
    "class tree:\n",
    "    def __init__(self, list_layers, loss_layer):\n",
    "        self.layers = list_layers\n",
    "        self.loss_layer = loss_layer\n",
    "        self.loss = 0\n",
    "        \n",
    "    def forward_pass(self, X ,y):\n",
    "        for layer in self.layers:\n",
    "            layer.forward_pass(X)\n",
    "            X = layer.forward['out']\n",
    "            \n",
    "        self.prediction = np.argmax(X, axis = 0)\n",
    "        \n",
    "        \n",
    "        self.loss_layer.forward_pass(X, y)\n",
    "        self.loss = self.loss_layer.forward['out']\n",
    "    \n",
    "    def backward_pass(self, X, y):\n",
    "        self.loss_layer.backward_pass()\n",
    "        grad = self.loss_layer.backward['t1']\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward_pass(grad)\n",
    "            grad = layer.backward['t1']\n",
    "    \n",
    "    def calculate_gradients(self):\n",
    "        for layer in self.layers:\n",
    "            layer.calculate_gradient()\n",
    "    \n",
    "    def update_parameters(self, epsilon):\n",
    "        for layer in self.layers:\n",
    "            layer.update_parameters(epsilon = epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data (MNIST) and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL MNIST DATASET \n",
    "#######################################################################\n",
    "X_train_vec = np.load(\"mnist.npz_FILES/train.npy\")/255\n",
    "y_train = np.squeeze(np.load(\"mnist.npz_FILES/train_labels.npy\"))\n",
    "temp_y = np.zeros([10,len(y_train)])\n",
    "temp_y[y_train.astype(int), np.arange(len(y_train))] = 1\n",
    "y_train = temp_y\n",
    "\n",
    "#shuffle the dataset\n",
    "index = (np.random.choice(60000,60000, replace=False))\n",
    "X_train_vec = X_train_vec[:,index]\n",
    "y_train = y_train[:,index]\n",
    "\n",
    "\n",
    "X_train = np.zeros((60000, 28, 28))\n",
    "for i in range(60000):\n",
    "    X_train[[i],:,:] = np.reshape(X_train_vec[:,i], [28,28])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_test_vec = np.load(\"mnist.npz_FILES/test.npy\")/255\n",
    "y_test = np.squeeze(np.load(\"mnist.npz_FILES/test_labels.npy\"))\n",
    "temp_y = np.zeros([10,len(y_test)])\n",
    "temp_y[y_test.astype(int), np.arange(len(y_test))] = 1\n",
    "y_test = temp_y\n",
    "\n",
    "X_test = np.zeros((10000, 28, 28))\n",
    "for i in range(10000):\n",
    "    X_test[[i],:,:] = np.reshape(X_test_vec[:,i], [28,28])\n",
    "\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This allows us to implement convolution as a matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(A, b, stepsize=1):\n",
    "    # Parameters\n",
    "    #rollaxis first\n",
    "    A = np.rollaxis(A, 0, 3)\n",
    "    \n",
    "    M,N,depth,batch = A.shape\n",
    "    col_extent = N - b[2] + 1\n",
    "    row_extent = M - b[1] + 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get Starting block indices\n",
    "    start_idx = np.arange(b[1])[:,None]*N + np.arange(b[2])\n",
    "\n",
    "    # Get offsetted indices across the height and width of input array\n",
    "    offset_idx = np.arange(row_extent)[::stepsize,None]*N + np.arange(col_extent)[::stepsize]\n",
    "    \n",
    "    #get offsetted indices across the depth of input array\n",
    "    offset_depth = np.arange(b[0])[None,None,:]*M*N \n",
    "    \n",
    "    #get offsetted indices across the different images\n",
    "    offset_images = np.arange(batch)[None,None,None,:]*(M*N*depth)\n",
    "    \n",
    "    # Get all actual indices & index into input array for final output\n",
    "    indices = (start_idx.ravel()[:,None] + offset_idx.ravel()[::1])[:,:,None]\n",
    "    indices = (indices + offset_depth)[:,:,:,None]\n",
    "\n",
    "\n",
    "    indices = indices + offset_images\n",
    "    output = np.take (A.ravel('F'), indices)\n",
    "\n",
    "    return np.rollaxis(output,2,0 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are three different kinds of convolutions we need to consider:\n",
    "\n",
    "1) Forward convolution\n",
    "\n",
    "2) Backward convolution (for the images)\n",
    "\n",
    "3) Backward convolution (for the weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_zeros_4d(image, zeros):\n",
    "    #zeros is the thickness of zeros on each side(x, y). scalar.\n",
    "    #image is a 3D tensor\n",
    "    depth, I,J, batch = image.shape\n",
    "    dim = 2*zeros + image.shape[1]\n",
    "    temp=np.zeros([depth,dim,dim,batch])\n",
    "    temp[:,zeros:(I+zeros), zeros:(J+zeros),:] = image\n",
    "    return temp\n",
    "def convolution_forward(Images, Kernels):\n",
    "    k_depth, k_size, k_size, k_num = Kernels.shape\n",
    "    \n",
    "    #mode is either valid or same\n",
    "    Images = pad_zeros_4d(Images, int((k_size-1)/2))\n",
    "    \n",
    "    \n",
    "    depth,M,N,batch = Images.shape    \n",
    "    \n",
    "    new_Image = im2col(Images, Kernels.shape[0:3]) #we don't need number of kernels\n",
    "    \n",
    "    Kernels = Kernels.reshape((k_depth, k_size**2, k_num),order = 'F')\n",
    "\n",
    "    output = np.einsum('ijkl,ijm->mkl', new_Image, Kernels)\n",
    "    \n",
    "    #reshaping the output to have proper dimensions (similar to input)\n",
    "    N_new = int(np.sqrt(output.shape[1]))\n",
    "    output = output.reshape((k_num, N_new, N_new,batch), order = 'F')\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def convolution_param(Images, in_grad):\n",
    "    #hardcode alert\n",
    "    Images = pad_zeros_4d(image=Images,zeros=1)\n",
    "    \n",
    "    #in_grad has shape (n_kernels X height X width X batch_size)\n",
    "    depth,M,N,batch = Images.shape  \n",
    "    n_ker, M_grad, N_grad, batch = in_grad.shape\n",
    "    \n",
    "    new_Image = im2col(Images, (depth, M_grad, N_grad))\n",
    "    \n",
    "    in_grad = in_grad.reshape((n_ker, M_grad * N_grad, batch),order = 'F')\n",
    "    \n",
    "    gradient = np.einsum('ijkl,mjl->ikm', new_Image, in_grad)\n",
    "    \n",
    "    gradient = gradient.reshape((depth, M-M_grad+1, N-N_grad+1, n_ker), order = 'F')\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def convolution_backward(in_grad, Kernels):\n",
    "\n",
    "    k_depth, k_size, k_size, k_num = Kernels.shape\n",
    "    #need to pad\n",
    "    in_grad = pad_zeros_4d(image=in_grad,zeros=int((k_size-1)/2))\n",
    "    \n",
    "    n_ker, M_grad,N_grad,batch = in_grad.shape\n",
    "    \n",
    "    in_grad = im2col(in_grad, (n_ker,k_size,k_size))\n",
    "    \n",
    "    Kernels = np.flip(np.flip(Kernels, 1), 2)\n",
    "    Kernels = Kernels.reshape((k_depth, k_size * k_size, k_num), order = 'F')\n",
    "    \n",
    "    gradient = np.einsum('ijkl,mji->mkl',in_grad, Kernels)\n",
    "    \n",
    "    gradient = gradient.reshape((k_depth, M_grad-k_size+1, N_grad-k_size+1, batch), order = 'F')\n",
    "    return gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are the layers required to construct the CNN, alongside with the cross entropy loss layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flatten_batch:\n",
    "    def __init__(self):\n",
    "        self.forward = {}\n",
    "        self.backward = {}\n",
    "        pass\n",
    "    def forward_pass(self, x):\n",
    "        self.shape = x.shape\n",
    "        batch_size = self.shape[-1]\n",
    "        self.forward[\"out\"] = x.reshape((-1,batch_size))\n",
    "    def backward_pass(self, in_grad):\n",
    "        self.backward[\"t1\"] = in_grad.reshape(self.shape)\n",
    "    def calculate_gradient(self):\n",
    "        pass\n",
    "    \n",
    "    def update_parameters(self, epsilon):\n",
    "        pass\n",
    "    \n",
    "\n",
    "class linear_layer_batch:\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.M = {}\n",
    "        self.M['W'] = parameter(data= 0.1*np.random.normal(size = [out_dim, in_dim]), \n",
    "                                gradient= np.zeros([out_dim, in_dim]))\n",
    "        self.M['b'] = parameter(data= 0.1*np.random.normal(size = [out_dim, 1]), \n",
    "                                gradient= np.zeros([out_dim, 1]))\n",
    "        self.forward = {}\n",
    "        self.backward = {}\n",
    "        self.X = 0\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        self.X = X\n",
    "        self.forward['f1'] = np.matmul(self.M['W'].data, self.X)\n",
    "        self.forward['out'] = self.forward['f1'] +  np.tile(self.M['b'].data, (1,self.forward['f1'].shape[-1] ) )\n",
    "        \n",
    "    def backward_pass(self, in_grad):\n",
    "        self.backward['t2'] = in_grad\n",
    "        self.backward['t1'] = np.matmul(self.M['W'].data.T, self.backward['t2'])\n",
    "    \n",
    "    def calculate_gradient(self):\n",
    "        self.M['W'].gradient =+ np.matmul(self.backward['t2'], self.X.T)\n",
    "        self.M['b'].gradient =+ np.sum(self.backward['t2'], axis = 1, keepdims= True)\n",
    "\n",
    "    \n",
    "    def update_parameters(self, epsilon):\n",
    "        for key in self.M:\n",
    "            \n",
    "            param = self.M[key]\n",
    "            \n",
    "            assert (param.gradient).shape == (param.data).shape\n",
    "            \n",
    "            param.data = param.data - epsilon * param.gradient/self.forward['f1'].shape[1]\n",
    "            #zero the gradients\n",
    "            param.gradient = np.zeros(np.shape(param.gradient))\n",
    "class relu_batch:\n",
    "    def __init__(self):\n",
    "        self.forward = {}\n",
    "        self.backward = {}\n",
    "        self.X = 0\n",
    "        self.prediction = 0\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        self.X = X\n",
    "        self.forward['out'] = np.maximum(self.X, 0)\n",
    "        \n",
    "    def backward_pass(self, in_grad):\n",
    "        self.backward['t1'] = (self.forward['out'] > 0)* in_grad\n",
    "    def calculate_gradient(self):\n",
    "        pass\n",
    "    \n",
    "    def update_parameters(self, epsilon):\n",
    "        pass\n",
    "\n",
    "def maxpool_batch(x, pool_size):\n",
    "    #x is a 4d array\n",
    "    depth, height, width, batch = np.shape(x)\n",
    "    x = im2col(A= x,b=(depth, pool_size, pool_size, batch), stepsize=2)\n",
    "    \n",
    "    x = np.max(x, axis = 1)\n",
    "    x = x.reshape((depth, int(height/2), int(width/2) ,batch),order = 'F')\n",
    "    return x\n",
    "\n",
    "\n",
    "class max_pool_batch:\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        self.backward = {}\n",
    "        self.forward = {}\n",
    "\n",
    "    def forward_pass(self, image):\n",
    "        self.in_size = image.shape\n",
    "        self.image = image\n",
    "        self.aug_size = int( np.ceil(self.in_size[1]/self.pool_size) )\n",
    "        \n",
    "        self.forward['out'] = maxpool_batch(image,self.pool_size)\n",
    "        self.aug_matrix = np.zeros(image.shape)\n",
    "        \n",
    "    def backward_pass(self, in_grad):\n",
    "        self.aug_matrix = np.repeat(np.eye(self.aug_size), \n",
    "                                    self.pool_size).reshape(self.aug_size, \n",
    "                                                            self.aug_size*self.pool_size)\n",
    "        \n",
    "        depth, height, width,batch = self.in_size\n",
    "        \n",
    "        self.backward[\"t2\"] = np.einsum(\"ij,njkm,kl->nilm\", \n",
    "                                        self.aug_matrix.T, \n",
    "                                        in_grad, \n",
    "                                        self.aug_matrix)[:,0:height, 0:width]\n",
    "        self.backward[\"out\"] = np.einsum(\"ij,njkm,kl->nilm\", \n",
    "                                        self.aug_matrix.T, \n",
    "                                        self.forward['out'], \n",
    "                                        self.aug_matrix)[:,0:height, 0:width]\n",
    "        self.backward[\"t1\"] = (abs(self.image - self.backward[\"out\"])<10e-8 )* self.backward[\"t2\"]\n",
    "    def calculate_gradient(self):\n",
    "        pass\n",
    "    \n",
    "    def update_parameters(self, epsilon):\n",
    "        pass\n",
    "\n",
    "    \n",
    "class entropylosswithlogits:\n",
    "    def __init__(self):\n",
    "        self.forward = {}\n",
    "        self.backward = {}\n",
    "        self.X = 0\n",
    "        self.y = 0\n",
    "    \n",
    "    def forward_pass(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.forward['f1'] = np.exp(self.X) / np.tile(np.sum(np.exp(self.X), axis=0), (10,1) )\n",
    "        self.forward['f2'] = -y * np.log(self.forward['f1'])\n",
    "        self.forward['out'] = np.sum(self.forward['f2'])\n",
    "        \n",
    "    def backward_pass(self):\n",
    "        self.backward['t1'] = self.forward['f1'] - self.y\n",
    "    def calculate_gradient(self):\n",
    "        pass\n",
    "    def update_parameters(self, epsilon):\n",
    "        pass\n",
    "\n",
    "class conv_layer_batch:\n",
    "    def __init__(self, num_kernels, kernel_size, depth):\n",
    "        self.M = {}\n",
    "        self.M['kernels'] = parameter(data=0.1*np.random.normal(size = [depth,kernel_size,kernel_size,num_kernels]),\n",
    "                                           gradient = np.zeros([depth ,kernel_size,kernel_size,num_kernels]))\n",
    "        self.kernel_size = kernel_size\n",
    "        self.forward = {}\n",
    "        self.backward = {}\n",
    "        \n",
    "    def forward_pass(self, image):\n",
    "        \n",
    "        #populate the forward tree\n",
    "        self.image = image\n",
    "        self.forward['out'] = convolution_forward(Images=image, Kernels=self.M['kernels'].data)\n",
    "            \n",
    "    def backward_pass(self, in_grad):\n",
    "        #populate the backward tree\n",
    "        self.in_grad = in_grad\n",
    "        self.backward['t1'] = convolution_backward(in_grad=in_grad, Kernels=self.M['kernels'].data)\n",
    "        \n",
    "    def calculate_gradient(self):\n",
    "        self.M['kernels'].gradient += convolution_param(Images=self.image, in_grad=self.in_grad)\n",
    "        \n",
    "    \n",
    "    def update_parameters(self, epsilon):\n",
    "        #remember to divide by batchsize\n",
    "        self.M['kernels'].data = self.M['kernels'].data - epsilon/self.image.shape[-1] * self.M['kernels'].gradient\n",
    "        #set to zero\n",
    "        self.M['kernels'].gradient = np.zeros(self.M['kernels'].gradient.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can train the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_1 = conv_layer_batch(num_kernels = 8, kernel_size = 3, depth = 1)\n",
    "conv_2 = conv_layer_batch(num_kernels = 4, kernel_size = 3, depth = 8)\n",
    "maxpool1 = max_pool_batch(pool_size=2)\n",
    "flatten1 = flatten_batch()\n",
    "linear1 = linear_layer_batch(in_dim= (4*(14**2)) ,out_dim=32)\n",
    "relu1 = relu_batch()\n",
    "linear2 = linear_layer_batch(in_dim= 32, out_dim = 10)\n",
    "\n",
    "\n",
    "ELWL = entropylosswithlogits()\n",
    "\n",
    "model1 = tree([conv_1, conv_2, maxpool1, flatten1, linear1 ,relu1, linear2], ELWL)\n",
    "\n",
    "batch_size = 10\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    num_batches = int(y_train.shape[1]/batch_size)\n",
    "    for i in range(num_batches):\n",
    "        start = batch_size*i\n",
    "        end = batch_size*(i+1)\n",
    "        X = np.rollaxis(X_train[start:end,:,:], 0, 3)[None,:,:,:]\n",
    "        y = y_train[:,start:end]\n",
    "        \n",
    "        model1.forward_pass(X = X, y = y)\n",
    "        model1.backward_pass(X = X, y = y)\n",
    "        model1.calculate_gradients()\n",
    "        loss = loss + model1.loss\n",
    "           \n",
    "        if i%20 == 19:\n",
    "            print(np.sum(conv_1.M['D_Kernel'].gradient))\n",
    "            print(\"epoch: \",epoch,',', i ,\" out of \", num_batches, \" batches completed, loss = \", loss/(100*batch_size))\n",
    "            loss = 0\n",
    "        model1.update_parameters(epsilon = 0.01) \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a function to test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, X_test, Y_test,batch_size):\n",
    "    count = 0\n",
    "    Y_test = Y_test.T\n",
    "    for i in range(100):\n",
    "        start = batch_size*i\n",
    "        end = batch_size*(i+1)\n",
    "        X = np.rollaxis(X_test[start:end,:,:], 0, 3)[None,:,:,:]\n",
    "        y = Y_test[:,start:end]\n",
    "        model.forward_pass(X, y)\n",
    "        y_pred = model.prediction\n",
    "\n",
    "        \n",
    "        correct = np.argmax(y, axis = 0)\n",
    "\n",
    "        count += np.sum(correct == y_pred)\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: CNN, except now with separable convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward_sep(Images, D_Kernels, P_Kernels):\n",
    "    k_depth, k_size, k_size = D_Kernels.shape\n",
    "    depth, M, N, batch = Images.shape\n",
    "    \n",
    "    Images = pad_zeros_4d(Images, int((k_size-1)/2))\n",
    "    \n",
    "    new_Image = im2col(Images, D_Kernels.shape)\n",
    "    D_Kernels = D_Kernels.reshape((k_depth, k_size**2),order = 'F')\n",
    "    #depthwise conv\n",
    "    output = np.einsum('ijkl,ij->ikl', new_Image, D_Kernels)\n",
    "    \n",
    "    output2 = np.reshape(output, (depth, M ,N, batch), order = 'F')\n",
    "    #pointwise conv\n",
    "    output = np.einsum('ijkl,mi->mjkl', output2, P_Kernels)\n",
    "    return(output2, output)\n",
    "\n",
    "def conv_backward_sep(In_grad, D_Kernels, P_Kernels):\n",
    "    n_ker, M,N, batch = In_grad.shape\n",
    "    k_depth, k_size, k_size = D_Kernels.shape\n",
    "    n_ker, depth = P_Kernels.shape\n",
    "    \n",
    "    \n",
    "    out_grad_2 = np.einsum('ijkl,in->njkl', In_grad, P_Kernels)\n",
    "\n",
    "    out_grad = pad_zeros_4d(out_grad_2, int((k_size-1)/2))\n",
    "    out_grad = im2col(out_grad, D_Kernels.shape)\n",
    "    \n",
    "    D_Kernels = np.flip(np.flip(D_Kernels, 1),2)\n",
    "    D_Kernels = D_Kernels.reshape((depth, k_size**2), order = 'F')\n",
    "\n",
    "    out_grad = np.einsum('ijkl,ij->ikl',out_grad, D_Kernels)\n",
    "\n",
    "    out_grad = out_grad.reshape((k_depth,M,N,batch), order = 'F')\n",
    "    return(out_grad, out_grad_2)\n",
    "\n",
    "def conv_param_sep(Image, In_grad2, Image2, In_grad):\n",
    "    depth, M,N, batch = Image.shape\n",
    "    depth, M,N, batch = In_grad2.shape\n",
    "    depth, M,N, batch = Image2.shape\n",
    "    n_ker, M,N, batch = In_grad.shape\n",
    "    \n",
    "    P_grad = np.einsum('ijkl,mjkl->mi', Image2, In_grad)\n",
    "    \n",
    "    #hardcode alert\n",
    "    Image = pad_zeros_4d(Image, 1)\n",
    "    Image = im2col(Image, In_grad2.shape)\n",
    "    In_grad2 = In_grad2.reshape((depth, M*N, batch), order = 'F')\n",
    "    \n",
    "    \n",
    "    D_grad = np.einsum('ijkl,ijl->ik', Image, In_grad2)\n",
    "    \n",
    "    #hardcode alert\n",
    "    D_grad = D_grad.reshape((depth, 3, 3), order = 'F')\n",
    "    \n",
    "    return(P_grad, D_grad)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class conv_layer_sep_batch:\n",
    "    def __init__(self, num_kernels, kernel_size, depth):\n",
    "        self.M = {}\n",
    "        self.M['D_Kernels'] = parameter(data=0.5*np.random.normal(size = [depth,kernel_size,kernel_size]),\n",
    "                                           gradient = np.zeros([depth ,kernel_size,kernel_size]))\n",
    "        self.M['P_Kernels'] = parameter(data=0.5*np.random.normal(size = [num_kernels, depth]),\n",
    "                                       gradient = np.zeros([num_kernels,depth]))\n",
    "        self.kernel_size = kernel_size\n",
    "        self.forward = {}\n",
    "        self.backward = {}\n",
    "        \n",
    "    def forward_pass(self, image):\n",
    "        \n",
    "        #populate the forward tree\n",
    "        self.image = image\n",
    "        self.forward['f1'],self.forward['out'] = conv_forward_sep(image, \n",
    "                                                                  self.M['D_Kernels'].data, \n",
    "                                                                  self.M['P_Kernels'].data)\n",
    "    def backward_pass(self, in_grad):\n",
    "        #populate the backward tree\n",
    "        self.in_grad = in_grad\n",
    "        self.backward['t1'],self.backward['t2'] = conv_backward_sep(in_grad, \n",
    "                                                                    self.M['D_Kernels'].data, \n",
    "                                                                    self.M['P_Kernels'].data)\n",
    "    def calculate_gradient(self):\n",
    "        #note that there is no gradient accum here\n",
    "        self.M['P_Kernels'].gradient,self.M['D_Kernels'].gradient   =  conv_param_sep(self.image, \n",
    "                                                                                      self.backward['t2'], \n",
    "                                                                                      self.forward['f1'], \n",
    "                                                                                      self.in_grad)\n",
    "    def update_parameters(self, epsilon):\n",
    "        #remember to divide by batchsize\n",
    "        self.M['D_Kernels'].data -= epsilon * self.M['D_Kernels'].gradient\n",
    "        self.M['P_Kernels'].data -= epsilon * self.M['P_Kernels'].gradient\n",
    "        #set to zero\n",
    "        self.M['D_Kernels'].gradient = np.zeros(self.M['D_Kernels'].gradient.shape)\n",
    "        self.M['P_Kernels'].gradient = np.zeros(self.M['P_Kernels'].gradient.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_1 = conv_layer_sep_batch(num_kernels = 8, kernel_size = 3, depth = 1)\n",
    "conv_2 = conv_layer_sep_batch(num_kernels = 4, kernel_size = 3, depth = 8)\n",
    "maxpool1 = max_pool_batch(pool_size=2)\n",
    "flatten1 = flatten_batch()\n",
    "linear1 = linear_layer_batch(in_dim= (4*(14**2)) ,out_dim=32)\n",
    "relu1 = relu_batch()\n",
    "linear2 = linear_layer_batch(in_dim= 32, out_dim = 10)\n",
    "\n",
    "\n",
    "ELWL = entropylosswithlogits()\n",
    "\n",
    "model1 = tree([conv_1, conv_2,  maxpool1, flatten1, linear1 ,relu1, linear2], ELWL)\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    num_batches = int(y_train.shape[1]/batch_size)\n",
    "    for i in range(num_batches):\n",
    "        start = batch_size*i\n",
    "        end = batch_size*(i+1)\n",
    "        X = np.rollaxis(X_train[start:end,:,:], 0, 3)[None,:,:,:]\n",
    "        y = y_train[:,start:end]\n",
    "        \n",
    "        model1.forward_pass(X = X, y = y)\n",
    "        model1.backward_pass(X = X, y = y)\n",
    "        model1.calculate_gradients()\n",
    "        loss = loss + model1.loss\n",
    "           \n",
    "        if i%20 == 19:\n",
    "            print(np.sum(conv_1.M['P_Kernels'].gradient))\n",
    "            print(\"epoch: \",epoch,',', i ,\n",
    "                  \" out of \", num_batches, \n",
    "                  \" batches completed, loss = \", loss/(100*batch_size))\n",
    "            loss = 0\n",
    "        model1.update_parameters(epsilon = 0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model1, X_test=X_test, Y_test=y_test.T,batch_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: Separable convolutions with depth filters being an outer product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = np.arange(5*36*2).reshape((5,6,6,2))\n",
    "vec1 = np.arange(5*3).reshape((5,3))\n",
    "vec2 = np.arange(5*3).reshape((5,3))\n",
    "P_Kernels = np.arange(4*5).reshape((4,5))\n",
    "b = np.array([5,3,3])\n",
    "\n",
    "class conv_layer_new_batch:\n",
    "    def __init__(self, num_kernels, kernel_size, depth):\n",
    "        self.M = {}\n",
    "        self.M['vec1'] = parameter(data=0.5*np.random.normal(size = [depth,kernel_size]),\n",
    "                                           gradient = np.zeros([depth ,kernel_size]))\n",
    "        self.M['vec2'] = parameter(data=0.5*np.random.normal(size = [depth,kernel_size]),\n",
    "                                           gradient = np.zeros([depth ,kernel_size]))\n",
    "        self.M['P_Kernels'] = parameter(data=0.5*np.random.normal(size = [num_kernels, depth]),\n",
    "                                       gradient = np.zeros([num_kernels,depth]))\n",
    "        self.kernel_size = kernel_size\n",
    "        self.forward = {}\n",
    "        self.backward = {}\n",
    "        \n",
    "    def forward_pass(self, image):\n",
    "        \n",
    "        #populate the forward tree\n",
    "        self.image = image\n",
    "        D_Kernels = np.einsum( 'ij,ik->ijk',self.M['vec1'].data,self.M['vec2'].data)\n",
    "        self.forward['f1'],self.forward['out'] = conv_forward_sep(image, \n",
    "                                                                  D_Kernels, \n",
    "                                                                  self.M['P_Kernels'].data)\n",
    "    def backward_pass(self, in_grad):\n",
    "        #populate the backward tree\n",
    "        self.in_grad = in_grad\n",
    "        D_Kernels = np.einsum( 'ij,ik->ijk',self.M['vec1'].data,self.M['vec2'].data)\n",
    "        self.backward['t1'],self.backward['t2'] = conv_backward_sep(in_grad, \n",
    "                                                                    D_Kernels, \n",
    "                                                                    self.M['P_Kernels'].data)\n",
    "    def calculate_gradient(self):\n",
    "        #note that there is no gradient accum here\n",
    "        self.M['P_Kernels'].gradient, D_Kernels_grad = conv_param_sep(self.image, \n",
    "                                                                      self.backward['t2'], \n",
    "                                                                      self.forward['f1'], \n",
    "                                                                      self.in_grad)\n",
    "        self.M['vec1'].gradient = np.einsum('ijk,ik->ij', D_Kernels_grad, self.M['vec2'].data)\n",
    "        self.M['vec2'].gradient = np.einsum('ijk,ij->ik', D_Kernels_grad, self.M['vec1'].data)\n",
    "        \n",
    "    def update_parameters(self, epsilon):\n",
    "        #remember to divide by batchsize\n",
    "        self.M['P_Kernels'].data -= epsilon * self.M['P_Kernels'].gradient\n",
    "        self.M['vec1'].data -= epsilon * self.M['vec1'].gradient\n",
    "        self.M['vec2'].data -= epsilon * self.M['vec2'].gradient\n",
    "        #set to zero\n",
    "        self.M['P_Kernels'].gradient = np.zeros(self.M['P_Kernels'].gradient.shape)\n",
    "        self.M['vec1'].gradient = np.zeros(self.M['vec1'].gradient.shape)\n",
    "        self.M['vec2'].gradient = np.zeros(self.M['vec2'].gradient.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_1 = conv_layer_new_batch(num_kernels = 8, kernel_size = 3, depth = 1)\n",
    "conv_2 = conv_layer_new_batch(num_kernels = 4, kernel_size = 3, depth = 8)\n",
    "maxpool1 = max_pool_batch(pool_size=2)\n",
    "flatten1 = flatten_batch()\n",
    "linear1 = linear_layer_batch(in_dim= (4*(14**2)) ,out_dim=32)\n",
    "relu1 = relu_batch()\n",
    "linear2 = linear_layer_batch(in_dim= 32, out_dim = 10)\n",
    "\n",
    "\n",
    "ELWL = entropylosswithlogits()\n",
    "\n",
    "model1 = tree([conv_1, conv_2,  maxpool1, flatten1, linear1 ,relu1, linear2], ELWL)\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    num_batches = int(y_train.shape[1]/batch_size)\n",
    "    for i in range(num_batches):\n",
    "        start = batch_size*i\n",
    "        end = batch_size*(i+1)\n",
    "        X = np.rollaxis(X_train[start:end,:,:], 0, 3)[None,:,:,:]\n",
    "        y = y_train[:,start:end]\n",
    "        \n",
    "        model1.forward_pass(X = X, y = y)\n",
    "        model1.backward_pass(X = X, y = y)\n",
    "        model1.calculate_gradients()\n",
    "        loss = loss + model1.loss\n",
    "           \n",
    "        if i%100 == 99:\n",
    "            print(\"epoch: \",epoch,',', i ,\" out of \", num_batches, \" batches completed, loss = \", loss/(100*batch_size))\n",
    "            loss = 0\n",
    "        model1.update_parameters(epsilon = 0.01)     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
